---
title: "1.1.WildTraxDataExtraction"
author: "Angela Moreras"
date: "2024-02-20"
output: html_document
---

This document is based on the workflow produce by Elly Knight to produce BAM's National models v5, however, it was produce to be a more general version using the WildRTrax package (version 1.1.0, be aware that functions and arguments can change between versions) and limiting the data points to the extent of the Study Area. 
In this first section we are going to extract data from WildTrax. 
Note also that data is not standardize in a particular way in WildTrax nor in wildRtrax, and therefore, columns and the type of data of each column can change. 

To log in in the WildTrax one needs to have credentials. To avoid reveling sensitive information in the script credentials need to be loaded by each user using the Sys.setenv function in this format: 
Sys.setenv(WT_USERNAME = "guest", WT_PASSWORD = "Apple123") 
where the username and the password will be replaced by the credentials each person has.

It requires to download the wildRtrax package (to download it follow instruction in https://github.com/ABbiodiversity/wildRtrax)
 

Input objects:
-> bcr.StudyArea is a shapefile that contain the study area with BCR limits. 
Note it is run in 00.StudyAreaPrep.Rmd but loaded here

Output objects; 
->dat: raw wildtrax data
->project.report: report of projects included
->use.wt: dataset to use in further steps of workflow

#0 Prep

Load packages
```{r, echo=F, results = F}
library(tidyverse) #basic data wrangling
library(wildRtrax) #to download data from wildtrax
library(data.table) #for binding lists into dataframes
library(lubridate) #date wrangling
library(purrr) #Mapping functions to read in multiple files
library(sf) #read in & handle study area
library(terra) #raster management
library(dggridR) #to make grid for checking for duplicates
library(R.utils) #to create a timer for the downloading loop
#library(arsenal)
```
##0.0. Set folders

```{r}
root.drive <- "G:/Shared drives/BAM_NationalModels5/Data"
root<- "."
```

##0.1. Load study area and proper projections 
```{r}
projection.trans<-5072
projection.st_as_sf<-4326

bcr.StudyArea<- read_sf(paste0(root, "/Study_area/StudyArea_wrangled.shp"))

#Alternatively:
#load(file.path(root,"StudyArea.Rdata")) 
```


##0.2. Clean and standaridize wildtrax columns function
 
Given that different projects can have in a given column different types of data. 
There is need to set the right type for each column. 

Both sensor types are going to be compile in the same data frame to save a backup of the whole data being used.
However, for ARU we need to change the two columns to avoid conflicts with PC data. 
Additionally, both survey_date and recording_date_time are now compile in date column 

For ARU data also, all tasks with task_method "None" are going to be excluded

I also use this function to bring only the data we are interested in. 

NOTE: variables selected may differ depending in the data needed from wildtrax
Variables drop for the EHJV (in # the justification): 
* equipment_make        #all NA
* equipment_model       #all NA
* vocalization          #unnecessary information
* comments              #unnecessary information
* observer              #Sensitive data 

Finally we set WildTrax as source and extract the year the data was collected 

```{r, echo=FALSE}
w_clean_download<-function (rawdata){
  
  if (any(str_detect(colnames(rawdata),"aru"))){  #selecting ARU
    
    rawdata<-rawdata%>% 
      #establish column type
      mutate(across(c(organization, location, equipment_make, equipment_model, aru_task_status, task_duration, 
                      task_method, species_code, species_common_name, species_scientific_name, individual_count, 
                      vocalization, tag_is_verified, observer, species_individual_comments, task_comments), 
                    as.character),
             across(c(project_id, location_id, location_buffer_m, recording_id, task_id, individual_order, tag_id,
                      observer_id), 
                    as.integer),
             across(c(longitude,latitude,detection_time,tag_duration),
                    as.numeric),
             )%>%           
      #rename columns
      rename (date= recording_date_time, 
              aru_detection_time=detection_time)%>% 
      #select columns 
      dplyr::select(organization, project_id, location_id, longitude, latitude, recording_id, date, location_buffer_m,
              task_id, aru_task_status, task_duration,task_method, aru_detection_time, species_code,
              species_scientific_name, individual_order,individual_count, date, observer_id)%>% 
      #exclude task_method "None"
      filter(task_method!="None")
  }
  
  else { #selecting PC
    
    rawdata<-rawdata%>% 
      #establish column type
      mutate(across(c(organization, project, location, survey_url, observer, survey_duration_method, 
                      survey_distance_method, detection_distance, detection_time, species_code,
                      species_common_name,species_scientific_name, individual_count, detection_heard,
                      detection_seen,detection_comments),
                    as.character),
             across(c(project_id, location_id, location_buffer_m, survey_id), 
                    as.integer),
             across(c(longitude,latitude),
                    as.numeric),
             )%>%           
      #rename columns
      rename (date= survey_date)%>%   
      #select columns 
      dplyr::select(organization, project, project_id, location_id, longitude, latitude, survey_id, date, 
              survey_distance_method,survey_duration_method, detection_time, location_buffer_m,
              detection_distance, species_code, species_common_name, species_scientific_name,individual_count)
  }
  
  rawdata <- rawdata %>% 
  mutate(source = "WildTrax", 
          year = format(as.Date(date), format = "%Y"))    #Extract the year 
}

```

Data from WildTrax can be downloaded in two was: manually from their webpage or using wildRtrax. 
Currently is faster to dowload data manually, but here we have code for both options.
If data is download manually start in point 1 and skip point 2. 
If data is download using wildRtrax skip point 1 and start in point 2.

#1.Download data manually
On WildTrax webpage ARU files are also listed as PC. So when selecting to download PC ensure you only download PC files. 
If data is downloaded manually, please ensure that the folder keeps the same structure root + /wildTrax_manual, to the code works properly 
Note: files need to be uncompressed 

##1.1. List of files 

```{r}
wt.files <- list.files(file.path(root, "/wildTrax_manual"),recursive = TRUE, pattern="main_report")
```


##1.2. Loop through files to crop data to study area


```{r, results='hide'}

projects.report<-as.data.frame(matrix(ncol=2, nrow=length(wt.files)))
colnames(projects.report) <- c("project_id", "download")
dat.raw <- data.frame() 
  
for(i in 1:length(wt.files)){
  
  report.try <- read.csv(file.path(root, "wildTrax_manual", wt.files[i]), na.strings=c("NA", "")) #Read file 
  
  projects.report$project_id[i]<- report.try$project_id[1] #Create report with project_id
  
  report.try<-report.try[which(!is.na(report.try$longitude)),] %>% #get rid of NAs  
    w_clean_download()                                             #Set column type
  
  if (nrow(report.try)!=0){
    
    report.try.sh <- report.try %>% 
      distinct(location_id, .keep_all = TRUE)%>% #use only unique locations
      sf::st_as_sf(coords = c("longitude", "latitude"), crs=projection.st_as_sf) %>% 
                                                              #transform data to shape file
      st_transform(projection.trans) %>%        #Change projection to match
      st_crop(bcr.StudyArea)                           #Crop data point to the extent of study area
      
                                                                              
    if (nrow(report.try.sh)!=0){
      print (paste("in intersection", i))
      
      report.try.sh.re<-st_intersection(report.try.sh, bcr.StudyArea) #get rid of points outside 
                                                                #the Study Area
      if (nrow(report.try.sh.re)==0){
        projects.report$download[i]= "No match"
        next
      }
      
      projects.report$download[i]<- paste0("Match ", Sys.Date())
      
      unique.loc<- report.try.sh.re$location_id  #extract unique locations 
      #filter the original report using the unique locations
      report.try.re<- report.try %>% filter(location_id %in% unique.loc) %>% 
                       #Assign sensor 
                       mutate(sensor=ifelse(any(str_detect(colnames(report.try),"aru")),"ARU","PC"))                   
      dat.raw <- dat.raw%>% 
        bind_rows(report.try.re)
    }
    else{
      projects.report$download[i]= "No match"
    }    
  }
  
  else{
    projects.report$download[i]= "No match"
    next
  }    
  
  print(paste0("Finished dataset ", i, " of ", nrow(projects.report), " projects"))
}

```
##1.3. Missing projects?

From decompressing the downloaded data some files where impossible to extract so, I got the feeling that some projects were missing.To see which projects may be missing. We are going to log in Wildtrax and  bring the list of projects we have access to. Sensor_id= "PC" strangely also brings all ARU projects. 

```{r}
wt_auth() 

projects <- wt_get_download_summary(sensor_id = 'PC') %>% select( project, project_id, sensor, tasks, status)
```

However, this also helps me to create a full report of the projects, so I can include the name of the project and the sensor type.  
```{r}
full.projects<- projects%>%
  full_join(projects.report)
```
See which files are missing
```{r}
missing<-full.projects[which(is.na(full.projects$download)),]
```

If missing projects are important for your project download the remaining projects and added to dat

##1.4. Clean data
Get rid of the projects that are listed as "DO NOT USE" in the projectInventory file. 

```{r}
avoid<-read.csv (file.path(root.drive,"/ProjectInventory/projectInstructions.csv"))
avoid.projects<-avoid$project_id[which(avoid$instruction=="DO NOT USE")]
```

```{r}
dat.raw<-dat.raw%>%
  filter(!project_id %in% avoid.projects)

projects.clean<- full.projects%>%
  filter(!project_id %in% avoid.projects)
```

##1.5. Save raw data

Data is saved along with the date stamp

```{r, echo=FALSE}
save(dat.raw, projects.clean, file=paste0(root, "/Wildtrax/wildtrax_rawManual&projects_", Sys.Date(), ".Rdata"))
```


#2. Download data with wildRtrax

##2.1. Login to WildTrax

Do not forget to load your WildTrax credentials beforehand otherwise this chunk is not going to run

```{r}
wt_auth() 
```
##2.2. List of projects
In this point point counts and ARU are select by selecting the option sensor = PC. Select the relevant columns 

```{r}
projects <- wt_get_download_summary(sensor_id = 'PC') %>% select( project, project_id, sensor, tasks, status)
```

Get rid of the projects that are listed as "DO NOT USE" in the projectInventory file. 

```{r}
avoid<-read.csv (file.path(root.drive,"/ProjectInventory/projectInstructions.csv"))
avoid.projects<-avoid$project_id[which(avoid$instruction=="DO NOT USE")]

projects.clean<- projects[!projects$project_id %in% avoid.projects,]
```

```{r, echo=FALSE}
#To be used to find issues with some projects 

start.time <- Sys.time()
eh14_raw <- wt_download_report(
  project_id = 1385,
  sensor_id = "PC",
  report = "main",
  weather_cols = FALSE
)
end.time <- Sys.time()
time.taken <- round(end.time - start.time,2)
time.taken


eh14_clean<-w_clean_download(eh14_raw)

use.aru.sample <- eh14_raw%>%  
  wt_replace_tmtt() %>%
  wt_make_wide()
```
##2.3. Loop through projects to download data

Data points that are outside the study area are going to be discarded here first by filtering by latitude and longitude (using st_crop), and then by the extent of the study area (using st_intersection). This was done trying to reduce the computational time. 

NOTE: some projects have issues to be download given their size (i.e., too large) depending on your laptop and internet speed they will or not be downloaded.
After the loop is completed a report is going to be in the report.projects object to check which projects were downloaded and which ones need to be downloaded manually. 

The loop needed to be run for half of the data and then the rest, as the credentials get kill halfway through (around project #300).

```{r, results='hide'}
dat.raw <- data.frame()               #empty object
seconds.per.min<- 60               #seconds in one min
wait<-30                           #minutes to wait

projects.clean$download<-NA              #create the download column


for(i in 1:nrow(projects.clean)) {
  
  response<-withTimeout({         #timer to stop downloading after a set time
  report.try <- try(wt_download_report(project_id = projects.clean$project_id[3],  #Download wildtrax project
                                       sensor = projects.clean$sensor[3], 
                                       weather_cols = FALSE, 
                                       report = "main")
                    )
  }, timeout = seconds.per.min*wait)#time to elapse 

  if (is.character(response)){
    projects.clean$download[i]="Reached time limit"    #enter here if the time limit was reached
    print(paste0("Reached time limit: Finished dataset ", projects.clean$project[i], " : ", i, " of ", nrow(projects.clean), " projects"))
    next
  }
  if (!is.data.frame(report.try)){
    projects.clean$download[i]= "Not a data frame" #enter here if is not a data frame
    print(paste0("Not a data frame: Finished dataset ", projects.clean$project[i], " : ", i, " of ", nrow(projects.clean), " projects"))
    next
  }
  
  report.try<-report.try[which(!is.na(report.try$longitude)),] %>% #get rid of NAs  
    w_clean_download()                                             #Set column type
  if (nrow(report.try)==0) {
    projects.clean$download[i]= "No match"
    print(paste0("No match (first): Finished dataset ", projects.clean$project[i], " : ", i, " of ",
                 nrow(projects.clean), " projects"))
    next
  }   
  
  report.try.sh <- report.try %>% 
                    distinct(location_id, .keep_all = TRUE)%>% #use only unique locations
                    sf::st_as_sf(coords = c("longitude", "latitude"), crs=projection.st_as_sf) %>% 
                                                              #transform data to shape file
                    st_transform(projection.trans) %>%        #Change projection to match
                    st_crop(bcr.StudyArea)                    #Crop data point to the extent of study area
      
                                                                              
  if (nrow(report.try.sh)!=0){
      print (paste("Match", i))

      report.try.sh.re<-st_intersection(report.try.sh, bcr.StudyArea) #get rid of points outside 
                                                                #the Study Area
      if (nrow(report.try.sh.re)==0){
        projects.clean$download[i]= "No match"
        print(paste0("No match (third):Finished dataset ", projects.clean$project[i], " : ", i, " of ",
                     nrow(projects.clean), " projects"))
        next
      }
      
      projects.clean$download[i]<- paste0("Match ", Sys.Date())
      
      unique.loc<- as.numeric(report.try.sh.re$location_id) #change location_id to numeric 
      report.try.re<- report.try %>% filter(location_id %in% unique.loc) %>% 
        #filter the original report using the unique locations
                       mutate(sensor=projects.clean$sensor[i])                     #Assign sensor 
                                          
      if(projects.clean$sensor[i]=="ARU"){
        report.try.re<-report.try.re %>% 
          mutate(project=projects.clean$project[i])

      }
      
      dat.raw <- dat.raw%>% 
        bind_rows(report.try.re)
  }
  
  else{
    projects.clean$download[i]= "No match"
    print(paste0("No match(second):Finished dataset ", projects.clean$project[i], " : ", i, " of ", nrow(projects.clean), " projects"))
    next
  }    
  
  print(paste0("Match: Finished dataset ", projects.clean$project[i], " : ", i, " of ", nrow(projects.clean), " projects"))
}

projects.clean$download %>% replace_na("Manually download")
```
It is possible that over(test.points, oceans) is a faster way to crop the points I did not test it. Maybe in the second round of downloading


Show the report of the projects
```{r}
report.projects<-projects.clean%>% select (project, project_id, download)

head (report.projects)
```

##2.4. Save raw data

Data is saved along with the date stamp

```{r, echo=FALSE}
save(dat.raw, projects, file=paste0(root, "/wildtrax_raw&projects_", Sys.Date(), ".Rdata"))
```

#3. Wrangle

Note that for the National Models there were entries for the equiment_make column, that refers to the equipment use to collect the data (Double check that this is the case); however, there is no data entry for the EHJV project, therefore, that part of point 5 is not followed here.  

```{r}
load(file.path(root,"WildTrax/", "wildtrax_rawManual&projects_2024-06-26.Rdata"))
```

##3.1. Remove ARU task with bad weather.and malfunction and set date to the proper format

```{r}
dat.filtered<-dat.raw%>%
  filter(!aru_task_status %in% c("Bad Weather", "Malfunction"))%>%
  mutate(date=ymd_hms(date))
```
  

##3.2. Replace tmtt

Fix the individual count label as tmtt (i.e., too many to tag) in the ARU sensor using the wt_replace_tmtt function which considers the observer_id. 
NOTE: In my case NAs are introduce when changing individual_count to integer as I have some CI 1, 2, 3.  

```{r}
dat.tmtt <- dat.filtered %>%
  mutate(recording_date_time=date) %>%            #to ensure wildRtrax is going to run
  wt_replace_tmtt()%>%                            #replace tmmtt
  select(-recording_date_time) %>%                #drop the column 
  mutate(across(individual_count, as.integer))%>% #change individual_count to integer
  filter(!is.na(individual_count))                #get rid of NAs
```

##3.2. Caclulate duration and distance

Here we extract the maximum survey distance and duration. Now we extract both values are now going to be sorted in two new columns called duration and distance along with the duration of the ARU task in min rather than seconds. 
Also we fix the task_method so that all PC data points are no longer NA but "PC". Finally we fix the location_buffer_m so all NAs are transform to 0. 

This may take a little while if the dataset is big. 

```{r}
dat.wt <- dat.tmtt %>%  
  rowwise() %>%       #to iterate over data frame
  mutate(distance_replace= gsub("m","", survey_distance_method), #replace m
         distance_max= ifelse(any(str_detect(distance_replace,"INF")), Inf, #is it INF?
                              as.integer(tail(unlist(strsplit(distance_replace,"-", " ")), n=1))), #select the last number
         distance= ifelse(is.na(distance_max), Inf, distance_max),
         duration_replace= gsub("min","", survey_duration_method), #replace min
         duration_max= as.integer(tail(unlist(strsplit(duration_replace,"-", " ")), n=1)),   #select the last number
         duration=ifelse(is.na(duration_max), as.numeric(gsub("s","", task_duration))/60, duration_max),
         tag_method=ifelse(is.na(task_method) & sensor=="PC", "PC", task_method),
         location_buffer_m=ifelse(is.na(location_buffer_m), 0, location_buffer_m)) %>%
  dplyr::select(-distance_replace, -duration_replace, -task_method) #Drop the unnecessary variables
```

##3.3. Calculate abundance and select columns

Here it is important to highlight that PC individual_count is the abundance for each location; however, the abundance is more complicated. ARU recordings can by transcribe by two different methods: 1 tag per individual per species per minute (i.e., 1SPM) and 1 tag per individual per species per task (i.e., 1SPT). This means that for 1SPM a new row is added per individual per species per minute (if it is heard in the subsequent minutes). Moreover, the ARU observer can establish in the individual_count column how many individuals are in each detection (box detection, for more details see https://wildtrax.ca/resources/user-guide/#acoustic-data), but the individual_order column keeps the record of how many individuals of the same species where heard in the recording. In other words, if you hear two Ovenbirds (OVEN), the first individual would be tagged as “1” and the second OVEN as “2”, and therefore the abundance is 2. Yet because we have some case where the individual_order does not reflect all the individual that are in the individual_count, as well as the tmtt cases, we need to sum those.  


Here we also select the columns of interest for the next steps of the modeling workflow. 
  
```{r}
use.wt<-dat.wt%>%
  filter(individual_count!=0)%>%                                #exclude zero counts
  group_by(organization, project_id, sensor, location_id, recording_id, date, task_id, tag_method, observer_id,
           species_code) %>%                                    #group to find the abundance
  distinct(across(-aru_detection_time))%>%#one unique entry per individual
  mutate(abundance= ifelse(tag_method=="PC", individual_count,  #calculate abundance
                           sum(individual_count))) %>%           
  ungroup()%>%                                                  #ungroup 
  group_by(organization, project_id, sensor, location_id, recording_id, date, task_id, tag_method, observer_id) %>%        
  distinct(species_code, .keep_all = TRUE)%>%                   #group and eliminate duplicates
  ungroup()%>%                                                  #ungroup                                 
  dplyr::select (source, organization, project_id, sensor, tag_method, location_id, location_buffer_m, latitude, 
          longitude,year, date,  observer_id, duration, distance, species_code, abundance) #select columns
```

Alternative abundance calculation using WildRtrax. 
Both chunks will give you the abundance but this one is using the WildRtrax package. 
However, for me some projects get lost using the function. Therefore I use the chunk above to produce the abundances. 

Note that the object would be wide. 
```{r}
#dat.wt.fixed<-dat.wt%>%
 # mutate(task_method=tag_method,
  #       recording_date_time=date,
   #      location=location_id)

#use.wtx<- wt_make_wide(dat.wt.fixed)
```
#4. Save data

```{r, echo=FALSE}

save(use.wt, file=paste0(root, "/Wildtrax/Wildtrax_", Sys.Date(), "_Wrangled.Rdata"))
```